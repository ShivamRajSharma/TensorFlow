{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as alb \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Conv2DTranspose, Dense, BatchNormalization,\n",
    "    GlobalAveragePooling2D, MaxPooling2D, LeakyReLU,\n",
    "    Dropout, Input, Reshape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_device = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_device[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "image_shape = (64, 64, 3)\n",
    "augmentation = alb.Compose([\n",
    "        alb.CenterCrop(160, 160),\n",
    "        alb.Resize(image_shape[0], image_shape[1], always_apply=True),\n",
    "        alb.Normalize(mean, std, max_pixel_value=255.0, always_apply=True)  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(batch_image_path):\n",
    "    batch_images = []\n",
    "    for path in batch_image_path:\n",
    "        full_path = os.path.join(faces_path, path)\n",
    "        img = np.array(Image.open(full_path).convert('RGB'))\n",
    "        img = augmentation(image=img)['image']\n",
    "        batch_images.append(img)\n",
    "    batch_images = np.array(batch_images)\n",
    "    return batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x):\n",
    "    a = x\n",
    "    for i in range(3):\n",
    "        x = Conv2DTranspose(x.shape[-1]*2, strides=2, kernel_size=7, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "    a_ = Conv2DTranspose(x.shape[-1], strides=8, kernel_size=3)(a)\n",
    "    a_ = BatchNormalization()(a_)\n",
    "    a_ = LeakyReLU()(a_)\n",
    "    x = tf.math.add(a_, x )\n",
    "    x = Conv2DTranspose(a.shape[-1], kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inp_shape, image_shape):\n",
    "    inp_ = Input(shape=(inp_shape))\n",
    "    x = Dense(2*2*2, use_bias=False)(inp_)\n",
    "    x = Reshape((2, 2, 2))(x)\n",
    "    x = Conv2DTranspose(32, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    for i in range(1):\n",
    "        x = residual_block(x)\n",
    "    x = Conv2DTranspose(3, kernel_size=4, strides=1, padding='same', use_bias=False, activation='tanh')(x)\n",
    "    model = Model(inp_, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(x):\n",
    "    a = Conv2D(x.shape[-1], kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    a = LeakyReLU()(a)\n",
    "    \n",
    "    b = Conv2D(x.shape[-1], kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    b = LeakyReLU()(b)\n",
    "    b = Conv2D(x.shape[-1], kernel_size=3, padding='same', use_bias=False)(b)\n",
    "    b = LeakyReLU()(b)\n",
    "    \n",
    "    c = Conv2D(x.shape[-1], kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    c = LeakyReLU()(c)\n",
    "    c = Conv2D(x.shape[-1], kernel_size=5, padding='same', use_bias=False)(c)\n",
    "    c = LeakyReLU()(c)\n",
    "    \n",
    "    d = MaxPooling2D(pool_size=3, strides=1, padding='same')(x)\n",
    "    d = Conv2D(x.shape[-1], kernel_size=1, padding='same', use_bias=False)(d)\n",
    "    d = LeakyReLU()(d)\n",
    "    \n",
    "    x = tf.concat([a, b, c, d], axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(image_shape):\n",
    "    inp_ = Input(shape=image_shape)\n",
    "    x = Conv2D(32, kernel_size=3, padding='same', use_bias=False)(inp_)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    for i in range(2):\n",
    "        x = inception_block(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "#         x = Dropout(0.3)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(100)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inp_, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    inp_ = Input(shape=(z_dims))\n",
    "    x = generator(inp_)\n",
    "    x = discriminator(x)\n",
    "    model = Model(inp_, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dims = 100\n",
    "image_shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = discriminator(image_shape)\n",
    "discriminator_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=2e-4, beta_1=0.5, beta_2=0.999), metrics=['accuracy'])\n",
    "\n",
    "generator_model = generator(z_dims, image_shape)\n",
    "\n",
    "gan_model = gan(generator_model, discriminator_model)\n",
    "gan_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3, beta_1=0.5, beta_2=0.999), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generation(generator_model):\n",
    "    z = np.random.normal(0, 1, (7*7, 100))\n",
    "    images = generator_model.predict(z)\n",
    "    images = images*0.5 + 0.5\n",
    "    fig, axis = plt.subplots(7, 7, figsize=(10, 10))\n",
    "    \n",
    "    num = 0\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            axis[i, j].imshow(images[num, :, :, :])\n",
    "            num += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        steps = len(image_path)//batch_size\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        running_gan_loss = 0\n",
    "        for step in tqdm(range(steps)):\n",
    "            if (step+1)*batch_size > len(image_path):\n",
    "                batch_image_path = image_path[step*batch_size :]\n",
    "            else:\n",
    "                batch_image_path = image_path[step*batch_size: (step+1)*batch_size]\n",
    "            batch_images = data_loader(batch_image_path)\n",
    "            z = np.random.normal(0, 1, (batch_images.shape[0], 100))\n",
    "            \n",
    "            real_labels = np.ones((batch_images.shape[0], 1))*0.9\n",
    "            fake_labels = np.ones((batch_images.shape[0], 1))*0.0\n",
    "            \n",
    "            generated_image = generator_model.predict(z)\n",
    "            \n",
    "#             discriminator.trainable = True\n",
    "            \n",
    "            real_images_loss, accuracy_real = discriminator_model.train_on_batch(batch_images, real_labels)\n",
    "            fake_images_loss, accuracy_fake = discriminator_model.train_on_batch(generated_image, fake_labels)\n",
    "            \n",
    "            loss = (real_images_loss + fake_images_loss)/2\n",
    "            accuracy = (accuracy_real + accuracy_fake)/2\n",
    "            running_loss += loss\n",
    "            running_acc += accuracy\n",
    "            \n",
    "            z = np.random.normal(0, 1, (batch_images.shape[0], 100))\n",
    "            \n",
    "#             discriminator.trainable = False\n",
    "            \n",
    "            gan_loss, gan_accuracy = gan_model.train_on_batch(z, real_labels)\n",
    "            running_gan_loss += gan_loss\n",
    "            \n",
    "        np.random.shuffle(image_path)\n",
    "            \n",
    "        print(f'EPOCH {epoch} COMPLETE |  DISCRIMINATOR-LOSS = {running_loss/steps} | DISCRIMINATOR-ACC {running_acc/steps} | GAN-LOSS {running_gan_loss/steps}' )\n",
    "        image_generation(generator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_path = 'img_align_celeba'\n",
    "image_path = os.listdir(faces_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1582 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[128,512,64,64] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/batch_normalization_2/FusedBatchNormV3 (defined at <ipython-input-14-34c78fabbd17>:22) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_3468]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-854df075481c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-34c78fabbd17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch_size, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#             discriminator.trainable = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mreal_images_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mfake_images_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                                     class_weight)\n\u001b[1;32m   1694\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/shivam/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[128,512,64,64] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/batch_normalization_2/FusedBatchNormV3 (defined at <ipython-input-14-34c78fabbd17>:22) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_3468]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "lr_ = [0.1, 0.01, 0.001, 0.0005, 0.0001]\n",
    "for i in range(len(lr)):\n",
    "    discriminator_model = discriminator(image_shape)\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr_[i], beta_1=0.5, beta_2=0.999), metrics=['accuracy'])\n",
    "\n",
    "    generator_model = generator(z_dims, image_shape)\n",
    "\n",
    "    gan_model = gan(generator_model, discriminator_model)\n",
    "    gan_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr_[i], beta_1=0.5, beta_2=0.999), metrics=['accuracy'])\n",
    "    train(batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(0, 1, (1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generator_model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x*0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f86905bb668>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!killall -9 python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
